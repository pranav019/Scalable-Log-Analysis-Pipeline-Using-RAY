"""
main.py

Entry point for the Scalable Log Analysis Pipeline demo.

This script:
- Initializes Ray.
- Creates Actor instances (aggregator, detector, alerting).
- Generates simulated log batches.
- Ingests batches into Ray object store and dispatches parse tasks.
- Aggregates, detects anomalies, prints final report.
"""

from typing import List
import time
import logging
import argparse
import ray


from src.data_processing import LogGenerator, batch_logs
from src.core_logic import (
    parse_logs_remote,
    AggregatorActor,
    AnomalyDetectorActor,
    AlertingActor,
)
from src.utils import DEFAULT_CONFIG


# Configure simple logging
logging.basicConfig(level=logging.INFO)
# getting the logger and naming the logger as main
logger = logging.getLogger("main")


def run_demo(
    num_batches: int = 8,
    lines_per_batch: int = 2000,
    parse_parallelism: int = 8,
    demo_sleep: float = 0.5,
) -> None:
    """
    Run a demo of the log analysis pipeline.

    Args:
        num_batches: Number of simulated batches to generate.
        lines_per_batch: Approximate number of raw log lines per batch.
        parse_parallelism: Number of parallel parser tasks per batch.
        demo_sleep: Small sleep between steps for readability of logs.
    """
    logger.info("Starting Ray Demo....")
    # Initialize Ray (local mode by default)
    ray.init(ignore_reinit_error=True)
    logger.info("Ray Initialized")

    try:
        alerting = AlertingActor.options(name="alerting_actor").remote()
        aggreagtor = AggregatorActor.options(name="aggregator_actor").remote()
        detector = AnomalyDetectorActor.remote(
            aggreagtor=aggreagtor, alerting=alerting, config=DEFAULT_CONFIG["detector"]
        )

        # Create Generator
        generator = LogGenerator(seed=42)

        for batch_idx, raw_batch in enumerate(
            batch_logs(generator, num_batches, lines_per_batch)
        ):
            logger.info("Produced batch %d with %d lines. ", batch_idx, len(raw_batch))
            # Put raw batch into Ray object store (zero-copy-ish)
            raw_ref = ray.put(raw_batch)
            # Split batch across parse tasks
            slice_size = max(1, len(raw_batch) // parse_parallelism)
            # a list collecting Ray object references (future)
            parse_refs = []

            for i in range(parse_parallelism):
                start = i * slice_size  # calculating the starting index of the each log
                end = None if i == parse_parallelism - 1 else (i + 1) * slice
                # parsed ref/logs that were generated by the tasks
                parse_refs.append(parse_logs_remote.remote(raw_ref, start, end))
                # parsed_list_refs = [
                #     ObjectRef1,  # parsed logs batch 0
                #     ObjectRef2,  # parsed logs batch 1
                #     ObjectRef3   # parsed logs batch 2
                # ]

                # eg : start = 0 * 25 = 0
                # end = (0+1) * 25 = 25
                # Worker 0 parses logs [0:25]

            # Wait for parse results
            parse_list_refs = ray.get(parse_refs)
            for parse_refs in parse_list_refs:
                # ingest_parsed_batch
                # A method (function inside the AggregatorActor class).
                # Its job: take a batch of parsed logs and update the aggregator’s state (like counting errors, grouping by user, etc.).
                # .remote(...) :: Special Ray keyword for calling actor methods or tasks asynchronously.
                aggreagtor.ingest_parsed_batch.remote(parse_refs)

            # Let detector analyze the current state(async)
            detector.scan_and_alert.remote()
            time.sleep(demo_sleep)

        logger.info("Waiting for outside tasks to finish....")
        time.sleep(2.0)

        final_stats = ray.get(aggreagtor.get_snapshot.remote())
        recent_alerts = ray.get(alerting.get_alerts.remote(limit=20))

        logger.info("Final Stats snapshots keys : %s", list(final_stats.keys()))
        logger.info("Recent alerts (count=%d)", len(recent_alerts))
        for alert in recent_alerts:
            logger.info("ALERT: %s ", alert)
    finally:
        logger.info("Shutting down ray")
        ray.shutdown()


if __name__ == "main":
    # argparse is a Python standard library module.
    # Purpose → to handle command-line arguments for your Python program.
    parser = argparse.ArgumentParser(description="Run ray log pipeline demo.")
    parser.add_argument(
        "--batches", type=int, default=6, help="Number of log batches to generate."
    )
    parser.add_argument("--lines", type=int, default=2000, help="Lines per batch.")
    parser.add_argument(
        "--parallel", type=int, default=8, help="Parse parallelism per batch."
    )
    args = parser.parse_args()
    # parse_args() :: It reads the command-line arguments given to your Python script when you run it in the terminal.
    # It then parses them according to the rules you defined with parser.add_argument(...).
    # Finally, it returns an object (usually called args) that holds all the argument values as attributes.
    run_demo(
        num_batches=args.batches,
        lines_per_batch=args.lines,
        parse_parallelism=args.parallel,
    )
